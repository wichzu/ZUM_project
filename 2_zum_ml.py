# -*- coding: utf-8 -*-
"""2.ZUM_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApvLZF69W1FkpOXNv3hEyJyppO274lXW
"""

from google.colab import drive
drive.mount('/content/drive/')

!python -m pip install spacy==2.3.2 -q
#2.3.2,  żeby był w tym polski

# pomocnicze
import re
import numpy as np
import pandas as pd
import string
# wizualizacja
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# nltk - do preprocessingu
import nltk
from nltk.stem import WordNetLemmatizer
import spacy
# sklearn - modele do ML
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.calibration import CalibratedClassifierCV

path = '/content/drive/MyDrive/ZUM/labeled_tweets.csv'   
df2 = pd.read_csv(path, delimiter = ",", encoding='utf-8')
df2.head()

ax = df2.groupby('sentiment').count().plot(kind='bar', title='Distribution of data', legend=False)
ax.set_xticklabels(['Negative', 'Neutral','Positive'], rotation=0)

"""#podział na dane treningowe, testowe i walidacyjne"""

X = df2['cleaned_text'].values.astype('U')
y = df2['sentiment'].values.astype('U')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =42)
X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state =42)

#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.05, random_state =42)

"""#zamiana na wektory"""

vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=500000)
vectorizer.fit(X_train)
print('No. of feature_words: ', len(vectorizer.get_feature_names()))

X_train = vectorizer.transform(X_train)
X_test  = vectorizer.transform(X_test)

y_train = y_train.astype(int)
y_test = y_test.astype(int)
y_validation = y_validation.astype(int)

"""#Ewaluacja modelu"""

def model_Evaluate(model):
  # Predykcja danych na danych testowych
  y_pred = model.predict(X_test)

  # Wyświetlenie metryk ewaluacji na podstawie predykcji i ground truth (faktycznych etykiet)
  print(classification_report(y_test, y_pred))

  # Obliczamy i wyświetlamy confusion matrix
  cf_matrix = confusion_matrix(y_test, y_pred)
  categories = ['Negative', 'Neutral', 'Positive']
  group_names = ['True Neg', 'False Pos', 'False Neg','True Pos' ]
  group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]
  labels = [f'{v1}\n{v2}' for v1, v2 in zip(group_names, group_percentages)]
  labels = np.asarray(labels).reshape(2,2)
  sns.heatmap(cf_matrix, annot=True, cmap = 'Blues', xticklabels=["Negative", "Neutral", "Positive"], yticklabels=["Negative", "Neutral", "Positive"])
  plt.title(f'Confusion Matrix')
  plt.xlabel('PREDICTED')
  plt.ylabel('TRUE')
  plt.show()

"""NB/BERNOULLI"""

BNBmodel = BernoulliNB(alpha = 2)
y_pred1 = BNBmodel.fit(X_train, y_train)
model_Evaluate(BNBmodel)

"""Support Vector Classification"""

svm = LinearSVC(random_state=0)
clf = CalibratedClassifierCV(svm) 
y_pred2 = clf.fit(X_train, y_train)
model_Evaluate(clf)

"""Regresja logistyczna"""

LRmodel = LogisticRegression(max_iter = 1000)
LRmodel.fit(X_train, y_train)
model_Evaluate(LRmodel)
y_pred3 = LRmodel.predict(X_test)

"""#Krzywa ROC"""

#predict probabilities
pred_prob1 = BNBmodel.predict_proba(X_test)
pred_prob2 = clf.predict_proba(X_test)
pred_prob3 = LRmodel.predict_proba(X_test)

fpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)
fpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)
fpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label=1)

# roc curve for tpr = fpr 
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)

# auc scores
auc_score1 = roc_auc_score(y_test, pred_prob1[:,1])
auc_score2 = roc_auc_score(y_test, pred_prob2[:,1])
auc_score3 = roc_auc_score(y_test, pred_prob3[:,1])

print(auc_score1, auc_score2, auc_score3)

plt.style.use('seaborn')

# plot roc curves
plt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Bernoulli Naive Bayes')
plt.plot(fpr2, tpr2, linestyle='--',color='red', label='Linear Support Vector Classification')
plt.plot(fpr3, tpr3, linestyle='--',color='darkviolet', label='Logistic Regression')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
# title
plt.title('ROC curve')
# x label
plt.xlabel('False Positive Rate')
# y label
plt.ylabel('True Positive rate')

plt.legend(loc='best')
plt.savefig('ROC',dpi=300)
plt.show();