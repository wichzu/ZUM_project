# -*- coding: utf-8 -*-
"""1.ZUM_labeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KBwf4CpyxAwVdRHLv79zymx2vosMo_X8

#Instalacja Spacy
"""

!python -m pip install spacy==2.3.2 -q
#2.3.2,  żeby był w tym polski

!python -m spacy download pl_core_news_md
#!python -m spacy download en_core_web_md

"""#import bibliotek"""

# pomocnicze
import re
import numpy as np
import pandas as pd
import string
# wizualizacja
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# nltk - do preprocessingu
import nltk
from nltk.stem import WordNetLemmatizer
import spacy
# sklearn - modele do ML
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, classification_report

import tweepy as tw
import time
import multiprocessing
from sklearn.cluster import KMeans
import seaborn as sns
import matplotlib.pyplot as plt

# import potrzebnych bibliotek

import matplotlib.pyplot as plt
import seaborn as sns
import re
import string

#nltk
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('wordnet')

# system
import os
import shutil

# tf
import tensorflow as tf

#gensim
from gensim.models.phrases import Phrases, Phraser
from gensim.models import Word2Vec

#wordcloud 
from wordcloud import WordCloud 

#filter warnings
import warnings
warnings.filterwarnings('ignore')

"""#wczytanie danych"""

df1 = pd.read_csv('/content/drive/MyDrive/ZUM/tweets_data.csv', index_col=0).astype(str)
df1.columns = ['treść tweeta']
df1

np.sum(df1.isnull().any(axis=1))

print(f'Liczba kolumn: {len(df1.columns)}')
print(f'Liczba wierszy: {len(df1)}')

print('Liczba duplikatów:',df1.duplicated().sum())
print(df1[df1.duplicated(keep=False)].tail())

df1 = df1.drop_duplicates()

"""#czyszczenie"""

nlp = spacy.load('pl_core_news_md')
lemma = WordNetLemmatizer()
nltk.download('wordnet')
stopwordlist = nlp.Defaults.stop_words

emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags=re.UNICODE)

def clean_text(text):
    text = text.lower()
    text = re.sub('https?:\/\/[a-zA-Z0-9@:%._\/+~#=?&;-]*', ' ', text)
    text = re.sub('\$[a-zA-Z0-9]*', ' ', text)
    text = re.sub('\@[a-zA-Z0-9]*', ' ', text)
    text = re.sub('[^a-zA-Z\']', ' ', text)
    text = re.sub('\t', '', text)
    text = re.sub('\S*@\S*\s?', ' ',text)
    text = emoji_pattern.sub(r'', text)
    text = ' '.join( [w for w in text.split() if len(w)>1] )
    text = ' '.join([lemma.lemmatize(x) for x in nltk.wordpunct_tokenize(text) if x not in stopwordlist])
    text = [lemma.lemmatize(x,nltk.corpus.reader.wordnet.VERB) for x in nltk.wordpunct_tokenize(text) if x not in stopwordlist]
    return text

df1['clean_text'] = df1['treść tweeta'].apply(clean_text)
df1.dropna(subset = ["clean_text"], inplace=True)
df1['cleaned_text'] = df1['clean_text'].apply(lambda x:' '.join(x))

"""#labeling"""

sent = [row for row in df1['clean_text']]
phrases = Phrases(sent, min_count=1, progress_per=500000)
bigram = Phraser(phrases)
sentences = bigram[sent]

# inicjalizacja modelu word2vec

import multiprocessing

w2v_model = Word2Vec(min_count=4,
                     window=5,
                     size =300,
                     sample=1e-5, 
                     alpha=0.03, 
                     min_alpha=0.0007, 
                     negative=20,
                     seed= 42,
                     workers=multiprocessing.cpu_count()-1)


w2v_model.build_vocab(sentences, progress_per=50000)

w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)

w2v_model.save("/content/drive/MyDrive/ZUM/w2v_model")

word_vectors = Word2Vec.load("/content/drive/MyDrive/ZUM/w2v_model").wv

model = KMeans(n_clusters=3, max_iter=1000, random_state=42, n_init=50).fit(X=word_vectors.vectors.astype('double'))

positive_cluster_center = model.cluster_centers_[2]
negative_cluster_center = model.cluster_centers_[1]
neutral_cluster_center= model.cluster_centers_[0]

words = pd.DataFrame(word_vectors.index2entity)
words.columns = ['words']
words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])
words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))
words.cluster = words.cluster.apply(lambda x: x[0])

words['cluster_value'] = [1 if i==2 else 0 if i==0 else -1 for i in words.cluster]
words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)

words[words["cluster_value"]==-1].sort_values("closeness_score")

words[words["cluster_value"]==0].sort_values("closeness_score")

words[words["cluster_value"]==1].sort_values("closeness_score")

positive = ['dobry','miły','wspaniały','zabawny','korzystny','uroczy','kochany']
neutral = ['zespół','temu','czyn','poseł','robić','mówić','powiedział', 'nagranie']
negative= ['głupi','zły','niesympatyczny','nudny','zły','okropny', 'absurdalny', 'durny']
for i in positive:
    words.loc[words["words"]==i,"cluster_value"] = 1
    
for i in neutral:
    words.loc[words["words"]==i,"cluster_value"] = 0
    
for i in negative:
    words.loc[words["words"]==i,"cluster_value"] = -1

words[words["words"]=="okropny"]

emotion = {0: "neutral",
           1: "positive",
          -1: "negative"}

words["sentiments"] = words["cluster_value"].map(emotion)

fig = plt.gcf()
fig.set_size_inches(7,7)
colors = ["dodgerblue","lightskyblue","seagreen"]
df_pie=words["sentiments"].value_counts().reset_index()
plt.pie(df_pie["sentiments"], labels = df_pie["index"], radius=2, colors=colors, autopct="%1.1f%%")
plt.axis('equal')
plt.title("Dystrybucja sentymentów ", fontsize=20)
plt.show()
df_pie

from math import nan
def get_sentiments(x, sent_dict):
    total = 0
    count = 0
    test = x["clean_text"]
    for t in test:
        if words_dict.get(t):
            total += int(sent_dict.get(t))
        count+=1 
    if count == 0:
     avg = 0 
    else:
      avg = total/count

    #print(avg)
    sentiment = -1 if avg < -0.15 else 1 if avg > 0.15 else 0
    return sentiment

words_dict = dict(zip(words.words, words.cluster_value))

df1["sentiment"] = df1.apply(get_sentiments, args=(words_dict,), axis=1)

df1["sentiment"].value_counts()

emotion = {0: "neutral",
           1: "positive",
          -1: "negative"}

df1["sentiments_val"] = df1["sentiment"].map(emotion)
df_pie = df1["sentiments_val"].value_counts().reset_index()
fig = plt.gcf()
fig.set_size_inches(7,7)
colors = ["dodgerblue","lightskyblue","seagreen"]
plt.pie(df_pie["sentiments_val"],labels= df_pie["index"],radius=2,autopct="%1.1f%%", colors=colors)
plt.axis('equal')
plt.title("Dystrybucja sentymentu w recenzjach ", fontsize=20)
#plt.savefig("images/Sentiment_Distribution.png")
plt.show()
df_pie

df1.head

df1.to_csv( "/content/drive/MyDrive/ZUM/labeled_tweets.csv", index=False, encoding='utf-8-sig')